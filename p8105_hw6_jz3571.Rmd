---
title: "p8105_hw6_jz3571"
author: "Junyan Zhu"
date: "2022-11-29"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r ,message=FALSE}
library(tidyverse)
library(modelr)
```

## Problem 1

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

#### Draw 5000 bootstrap samples and estimate R-square for each
```{r, cache=TRUE}
bootstrap_df = 
  weather_df %>% 
  bootstrap(n = 5000, id = "strap_number")

rsquare_df = 
  bootstrap_df %>% 
  mutate(
    models = map(strap, ~ lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::glance)
    ) %>% 
  unnest(results) %>% 
  select(strap_number, r.squared)

```

#### Plot the distribution of estimated R-square 
```{r}
rsquare_df %>% 
  ggplot(aes(x = r.squared)) +
  geom_density() +
  labs(x = "R_squared estimates",
       y = "Density",
       title = "Distribution of R-squared estimates") +
  theme(plot.title = element_text(hjust = 0.5))
```

#### 95% CI for R-squared

```{r}
quantile(rsquare_df$r.squared, c(0.025, 0.975))
```

#### Produce estimates of log(beta_0 * beta_1) for each bootstrap sample

```{r, cache=TRUE}
log_df = 
  bootstrap_df %>% 
  mutate(
    models = map(strap, ~ lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::tidy)
    ) %>% 
  unnest(results) %>% 
  select(strap_number, term, estimate) %>% 
  pivot_wider(
    names_from = "term",
    values_from = "estimate"
  ) %>% 
  janitor::clean_names() %>% 
  rename(beta_0 = intercept, beta_1 = tmin) %>% 
  mutate(log_beta0_times_beta1 = log(beta_0 * beta_1))
```


#### Plot the distribution of estimates of log(beta_0 * beta_1)

```{r}
log_df %>% 
  ggplot(aes(x = log_beta0_times_beta1)) +
  geom_density() +
  labs(x = "log(beta_0 * beta_1)",
       y = "Density",
       title = "Distribution of estimates of log(beta_0 * beta_1)") +
  theme(plot.title = element_text(hjust = 0.5))

```

From the graphs pf distribution above, we can see the distribution of R square is basically normal and mean is about 0.91. Besides, the the plot of log(beta_0 * beta_1) shows that the value of the value of log(beta_0 * beta_1) is also normally distributed around the 2.03.

#### 95% CI for log(beta_0 * beta_1)

```{r}
quantile(log_df$log_beta0_times_beta1, c(0.025, 0.975))
```


## Problem 2

```{r}
homicide_df = read_csv("./data/homicide-data.csv")
```

#### Create a city_state variable and clean data

```{r}
homicide_clean = homicide_df %>% 
  mutate(city_state = str_c(city, state, sep = "_"),
         victim_age = as.numeric(victim_age),
         resolved = case_when(
           disposition == "Closed without arrest" ~ 0,
           disposition == "Open/No arrest" ~ 0,
           disposition == "Closed by arrest" ~ 1
         ) ) %>% 
  filter(victim_race %in% c("Black", "White"),
         city_state != "Tulsa_AL",
         city_state != "Dallas_TX",
         city_state != "Phoenix_AZ",
         city_state != "Kansas City_MO") %>% 
   select(city_state, resolved, victim_age, victim_race, victim_sex)
```

#### Fit a logistic regression for resolved vs unresolved for the city of Baltimore_MD

```{r}
baltimore_df = 
  homicide_clean %>% 
  filter(city_state =="Baltimore_MD") 

glm(resolved ~ victim_age + victim_race + victim_sex, data = baltimore_df, family = "binomial") %>% 
  broom::tidy() %>% 
  mutate(odd_ratio = exp(estimate),
         CI_lower = exp(estimate - 1.96*std.error),
         CI_upper = exp(estimate + 1.96*std.error)) %>% 
  select(term, odd_ratio, starts_with("CI")) %>% 
  knitr::kable(digit = 3)
```

#### Run glm for each of the cities in your dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims.

```{r}
all_glm = 
  homicide_clean %>% 
  nest(data = -city_state) %>% 
  mutate(
    models = map(.x = data, ~ glm(resolved ~ victim_age + victim_race + victim_sex, data = .x, family = "binomial")),
    results = map(models, broom::tidy)
    ) %>% 
  unnest(results) %>% 
  filter(term == "victim_sexMale") %>% 
  mutate(odd_ratio = exp(estimate),
         CI_lower = exp(estimate - 1.96*std.error),
         CI_upper = exp(estimate + 1.96*std.error)) %>% 
  select(city_state, odd_ratio, starts_with("CI"))
  
all_glm %>% 
  knitr::kable(digit = 3)
```

#### Plot he estimated ORs and CIs for each city

```{r}
all_glm %>% 
  mutate(city_state = fct_reorder(city_state, odd_ratio)) %>% 
  ggplot(aes(x = city_state, y = odd_ratio)) +
  geom_point() + 
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(
    title = "Estimates Odd Ratio and CIs for Each City",
    x = "City, State",
    y = "Odd Ratio",
    caption = "Data from the Washington Post") +
   theme(plot.title = element_text(hjust = 0.5))
```

## Problem 3

#### Load and clean the data for regression analysis

```{r}
birthweight_df = read_csv("./data/birthweight.csv")
```

```{r}
bw_clean = 
  birthweight_df %>% 
  janitor::clean_names() %>% 
  mutate(
    babysex = factor(babysex),
    babysex = fct_recode(babysex, 
                         male = "1", 
                         female = "2"),
    frace = factor(frace),
    frace = fct_recode(frace, 
                       white = "1", 
                       black = "2", 
                       asian = "3", 
                       puerto_rican = "4", 
                       other = "8"),
    malform = factor(malform),
    malform = fct_recode(malform, 
                         absent = "0", 
                         present = "1"),
    mrace = factor(mrace),
    mrace = fct_recode(mrace, 
                       white = "1", 
                       black = "2",
                       asian = "3",
                       puerto_rican = "4"))

# check missing values
sum(is.na(bw_clean))
```

The resulting dataset `bw_clean` contains 20 variables: babysex, bhead, blength, bwt, delwt, fincome, frace, gaweeks, malform, menarche, mheight, momage, mrace, parity, pnumlbw, pnumsga, ppbmi, ppwt, smoken, wtgain. This dataset has 4342 observations and 0 missing value. Numeric variables `babysex`, `frace`, `malform`, and `mrace` were converted into factor variables. 

#### Propose a regression model for birthweight

###### view the data

```{r}
bw_clean %>% 
  lm(bwt ~ ., data =.) %>% 
  broom::tidy()
```

###### use the both_way stepwise model selection
```{r}
stepwise_model = step(lm(bwt ~ . ,data = bw_clean), direction = "both", trace = FALSE)
summary(stepwise_model)
```

According to the results from both_way stepwise model selection , we have the optimal regression model: bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken. The multiple r-squared is 0.7181, and adjusted r-squared is 0.7173. The p value is smaller than 2.2e-16.

#### show a plot of model residuals against fitted values

```{r}
residual_plot =
  bw_clean %>% 
  add_predictions(stepwise_model) %>% 
  add_residuals(stepwise_model) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.5) +
  geom_smooth(formula = 'y ~ x',method = "lm", se = FALSE) +
  labs(title = "Fitted Values vs. Residuals",
       x = "Fitted Values", 
       y = "Residuals",) +
  theme(plot.title = element_text(hjust = 0.5))
  
```

From the graph, we could see a big cluster of points around fitted values equal 2000 - 4000 where the model residuals tend to evenly distributed around 0. 

#### Compare my model to two others

```{r}
model_1 = lm(bwt ~ blength + gaweeks, data = bw_clean)

model_2 = lm(bwt ~ bhead * blength * babysex, data = bw_clean)

```

```{r}
cv_df = 
  crossv_mc(bw_clean, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  ) %>% 
  mutate(
    model_1  = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    model_2  = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x)),
    stepwise_model  = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x ))) %>% 
  mutate(
    rmse_model_1 = map2_dbl(model_1, test, ~rmse(model = .x, data = .y)),
    rmse_model_2    = map2_dbl(model_2, test, ~rmse(model = .x, data = .y)),
    rmse_stepwise = map2_dbl(stepwise_model, test, ~rmse(model = .x, data = .y)))
```

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) %>% 
  mutate(model = fct_reorder(model, rmse)) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_violin() +
  labs(title = "Cross Validation Comparisons",
       x = "Model",
       y = "RMSE") +
  theme(plot.title = element_text(hjust = 0.5))
```

From the violin plot we can see that my model `stepwise_model` (bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken) has the lowest rmse than `model_1` (bwt ~ blength + gaweeks) and `model_2` (bwt ~ bhead * blength * babysex). Therefore, `stepwise_model` has the best performance in predicting birthweight among the three models.